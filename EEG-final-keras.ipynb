{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded data from cached archive.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn; seaborn.set()\n",
    "from cesium import datasets\n",
    "\n",
    "# Returns\n",
    "#     dict\n",
    "#         Dictionary with attributes:\n",
    "#             - times: list of (4096,) arrays of time values\n",
    "#             - measurements: list of (4096,) arrays of measurement values\n",
    "#             - classes: array of class labels for each time series\n",
    "#             - archive: path to data archive\n",
    "#             - header: path to header file\n",
    "\n",
    "# https://github.com/cesium-ml/cesium-data/tree/master/andrzejak\n",
    "eeg = datasets.fetch_andrzejak()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Group together classes (Z, O), (N, F), (S) as normal, interictal, ictal\n",
    "eeg[\"classes\"] = eeg[\"classes\"].astype(\"U16\") #  allocate memory for longer class names\n",
    "\n",
    "eeg[\"classes\"][np.logical_or(eeg[\"classes\"]==\"Z\", eeg[\"classes\"]==\"O\")] = \"Normal\"\n",
    "eeg[\"classes\"][np.logical_or(eeg[\"classes\"]==\"N\", eeg[\"classes\"]==\"F\")] = \"Interictal\"\n",
    "eeg[\"classes\"][eeg[\"classes\"]==\"S\"] = \"Ictal\"\n",
    "\n",
    "fig, ax = plt.subplots(1, len(np.unique(eeg[\"classes\"])), sharey=True)\n",
    "for label, subplot in zip(np.unique(eeg[\"classes\"]), ax):\n",
    "    i = np.where(eeg[\"classes\"] == label)[0][0]\n",
    "    subplot.plot(eeg[\"times\"][i], eeg[\"measurements\"][i])\n",
    "    subplot.set(xlabel=\"time (s)\", ylabel=\"signal\", title=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature       mean median amplitude\n",
      "channel          0      0         0\n",
      "0        -4.132048   -4.0     143.5\n",
      "1       -52.444716  -51.0     211.5\n",
      "2        12.705150   13.0     165.0\n",
      "3        -3.992433   -4.0     171.5\n",
      "4       -17.999268  -18.0     170.0\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from cesium import featurize\n",
    "features_to_use = [\"mean\",\n",
    "                   \"median\",\n",
    "                   \"amplitude\"]\n",
    "                   \n",
    "fset_cesium = featurize.featurize_time_series(times=eeg[\"times\"],\n",
    "                                              values=eeg[\"measurements\"],\n",
    "                                              errors=None,\n",
    "                                              features_to_use=features_to_use)\n",
    "print(fset_cesium.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "def mean_square_signal(t, m, e):\n",
    "    return np.mean(m ** 2)\n",
    "\n",
    "def abs_diffs_signal(t, m, e):\n",
    "    return np.sum(np.abs(np.diff(m)))\n",
    "\n",
    "def variance(t, m, e):\n",
    "    return np.var(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature  mean_square abs_diffs     variance\n",
      "channel            0         0            0\n",
      "0        1650.122773   46948.0  1633.048953\n",
      "1        5133.124725   61118.0  2382.676526\n",
      "2        2384.051989   51269.0  2222.631150\n",
      "3        2231.742495   75014.0  2215.802969\n",
      "4        2340.967781   52873.0  2016.994142\n"
     ]
    }
   ],
   "source": [
    "other_features = {\n",
    "    \"mean_square\": mean_square_signal,\n",
    "    \"abs_diffs\": abs_diffs_signal,\n",
    "    \"variance\": variance\n",
    "}\n",
    "\n",
    "fset_others = featurize.featurize_time_series(times=eeg[\"times\"], values=eeg[\"measurements\"],\n",
    "                                           errors=None,\n",
    "                                           features_to_use=list(other_features.keys()),\n",
    "                                           custom_functions=other_features)\n",
    "print(fset_others.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>amplitude</th>\n",
       "      <th>mean_square</th>\n",
       "      <th>abs_diffs</th>\n",
       "      <th>variance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>channel</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-4.132048</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>143.5</td>\n",
       "      <td>1650.122773</td>\n",
       "      <td>46948.0</td>\n",
       "      <td>1633.048953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-52.444716</td>\n",
       "      <td>-51.0</td>\n",
       "      <td>211.5</td>\n",
       "      <td>5133.124725</td>\n",
       "      <td>61118.0</td>\n",
       "      <td>2382.676526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.705150</td>\n",
       "      <td>13.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>2384.051989</td>\n",
       "      <td>51269.0</td>\n",
       "      <td>2222.631150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3.992433</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>171.5</td>\n",
       "      <td>2231.742495</td>\n",
       "      <td>75014.0</td>\n",
       "      <td>2215.802969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-17.999268</td>\n",
       "      <td>-18.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>2340.967781</td>\n",
       "      <td>52873.0</td>\n",
       "      <td>2016.994142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "feature       mean median amplitude  mean_square abs_diffs     variance\n",
       "channel          0      0         0            0         0            0\n",
       "0        -4.132048   -4.0     143.5  1650.122773   46948.0  1633.048953\n",
       "1       -52.444716  -51.0     211.5  5133.124725   61118.0  2382.676526\n",
       "2        12.705150   13.0     165.0  2384.051989   51269.0  2222.631150\n",
       "3        -3.992433   -4.0     171.5  2231.742495   75014.0  2215.802969\n",
       "4       -17.999268  -18.0     170.0  2340.967781   52873.0  2016.994142"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fset_all = pd.concat([fset_cesium, fset_others], axis=1, sort=False)\n",
    "fset_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>amplitude</th>\n",
       "      <th>mean_square</th>\n",
       "      <th>abs_diffs</th>\n",
       "      <th>variance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>channel</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <th>channel</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.659069</td>\n",
       "      <td>0.124352</td>\n",
       "      <td>0.127373</td>\n",
       "      <td>0.113178</td>\n",
       "      <td>0.136089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median</th>\n",
       "      <th>0</th>\n",
       "      <td>0.659069</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.220845</td>\n",
       "      <td>0.282635</td>\n",
       "      <td>0.299343</td>\n",
       "      <td>0.287812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amplitude</th>\n",
       "      <th>0</th>\n",
       "      <td>0.124352</td>\n",
       "      <td>0.220845</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.928803</td>\n",
       "      <td>0.919876</td>\n",
       "      <td>0.928998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_square</th>\n",
       "      <th>0</th>\n",
       "      <td>0.127373</td>\n",
       "      <td>0.282635</td>\n",
       "      <td>0.928803</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.934930</td>\n",
       "      <td>0.999808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abs_diffs</th>\n",
       "      <th>0</th>\n",
       "      <td>0.113178</td>\n",
       "      <td>0.299343</td>\n",
       "      <td>0.919876</td>\n",
       "      <td>0.934930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.934739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "feature                  mean    median amplitude mean_square abs_diffs  \\\n",
       "channel                     0         0         0           0         0   \n",
       "feature     channel                                                       \n",
       "mean        0        1.000000  0.659069  0.124352    0.127373  0.113178   \n",
       "median      0        0.659069  1.000000  0.220845    0.282635  0.299343   \n",
       "amplitude   0        0.124352  0.220845  1.000000    0.928803  0.919876   \n",
       "mean_square 0        0.127373  0.282635  0.928803    1.000000  0.934930   \n",
       "abs_diffs   0        0.113178  0.299343  0.919876    0.934930  1.000000   \n",
       "\n",
       "feature              variance  \n",
       "channel                     0  \n",
       "feature     channel            \n",
       "mean        0        0.136089  \n",
       "median      0        0.287812  \n",
       "amplitude   0        0.928998  \n",
       "mean_square 0        0.999808  \n",
       "abs_diffs   0        0.934739  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_matrix = fset_all.corr()\n",
    "corr_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_features(threshold): \n",
    "    corr_matrix_abs = corr_matrix.abs()\n",
    "\n",
    "    # List of all features\n",
    "    all_features = [x[0] for x in corr_matrix]\n",
    "    \n",
    "    # List all values with high correlation\n",
    "    high_corr_var = np.where(corr_matrix_abs > threshold)\n",
    "    high_corr_var = [(corr_matrix_abs.columns[x][0],corr_matrix_abs.columns[y][0]) \n",
    "                     for x,y in zip(*high_corr_var) if x!=y and x<y]\n",
    "\n",
    "    # Create list of unique first, second tuple\n",
    "    a = [x[0] for x in high_corr_var]\n",
    "    b = [x[1] for x in high_corr_var]\n",
    "\n",
    "    first = list(set(a))\n",
    "    second = list(set(b))\n",
    "\n",
    "    # Subtract second tuple from first to get list of features \n",
    "    # that have low or no correlation\n",
    "    low_corr = list(set(a) - set(b))\n",
    "\n",
    "    # Check if there was any features not included from the complete list\n",
    "    other_features = list(set(all_features) - set(a))\n",
    "\n",
    "    final_features = low_corr + other_features\n",
    "    return(final_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.5 =  ['mean', 'amplitude', 'median', 'variance']\n",
      "Threshold: 0.6 =  ['mean', 'amplitude', 'median', 'variance']\n",
      "Threshold: 0.75 =  ['amplitude', 'median', 'mean', 'variance']\n"
     ]
    }
   ],
   "source": [
    "features_5 = get_final_features(0.5)\n",
    "features_6 = get_final_features(0.6)\n",
    "features_75 = get_final_features(0.75)\n",
    "\n",
    "print(\"Threshold: 0.5 = \", features_5)\n",
    "print(\"Threshold: 0.6 = \", features_6)\n",
    "print(\"Threshold: 0.75 = \", features_75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataframes with just the features we want\n",
    "fset_5 = fset_all[features_5]\n",
    "fset_6 = fset_all[features_6]\n",
    "fset_75 = fset_all[features_75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(np.arange(len(eeg[\"classes\"])), random_state=0)\n",
    "\n",
    "def build_model(fset):\n",
    "    models = []\n",
    "    \n",
    "    model_lr = LogisticRegression(random_state=0, solver='lbfgs', \n",
    "                                  multi_class='multinomial').fit(fset.iloc[train], \n",
    "                                                                 eeg[\"classes\"][train])\n",
    "    \n",
    "    model_nb = GaussianNB().fit(fset.iloc[train], eeg[\"classes\"][train])\n",
    "    \n",
    "    model_knn = KNeighborsClassifier(3).fit(fset.iloc[train], eeg[\"classes\"][train])\n",
    "\n",
    "    model_rfc = RandomForestClassifier(n_estimators=128, max_features=\"auto\", \n",
    "                                       random_state=0).fit(fset.iloc[train], \n",
    "                                                           eeg[\"classes\"][train])\n",
    "\n",
    "    models = [model_lr, model_nb, model_knn, model_rfc, train, test]\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5 = build_model(fset_5)\n",
    "model_6 = build_model(fset_6)\n",
    "model_75 = build_model(fset_75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  375\n",
      "test:  125\n"
     ]
    }
   ],
   "source": [
    "print(\"train: \", len(model_5[4]))\n",
    "print(\"test: \", len(model_5[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, f1_score, roc_auc_score\n",
    "    \n",
    "def predict(fset, model):\n",
    "    preds_lr = model[0].predict(fset)\n",
    "    preds_nb = model[1].predict(fset)\n",
    "    preds_knn = model[2].predict(fset)\n",
    "    preds_rfc = model[3].predict(fset)\n",
    "    \n",
    "    print(\"\\nLogistic Regression: \")\n",
    "    print(\"confusion_matrix:\\n\", confusion_matrix(preds_lr[test], eeg[\"classes\"][test], labels=[\"Normal\", \"Interictal\", \"Ictal\"]))\n",
    "    print(\"training accuracy = {:.2%}, test accuracy = {:.2%}\".format(\n",
    "              accuracy_score(preds_lr[train], eeg[\"classes\"][train]),\n",
    "              accuracy_score(preds_lr[test], eeg[\"classes\"][test])))\n",
    "    print(\"training precision = {:.2%}, test precision = {:.2%}\".format(\n",
    "              precision_score(preds_lr[train], eeg[\"classes\"][train], average='weighted'),\n",
    "              precision_score(preds_lr[test], eeg[\"classes\"][test], average='weighted')))\n",
    "    print(\"training sensitivity = {:.2%}, test sensitivity = {:.2%}\".format(\n",
    "              recall_score(preds_lr[train], eeg[\"classes\"][train], average='weighted'),\n",
    "              recall_score(preds_lr[test], eeg[\"classes\"][test], average='weighted')))\n",
    "    print(\"training f1_score = {:.2%}, test f1_score = {:.2%}\".format(\n",
    "              f1_score(preds_lr[train], eeg[\"classes\"][train], average='weighted'),\n",
    "              f1_score(preds_lr[test], eeg[\"classes\"][test], average='weighted')))\n",
    "   \n",
    "    print(\"\\nNaive Bayes: \")\n",
    "    print(\"confusion_matrix:\\n\", confusion_matrix(preds_nb[test], eeg[\"classes\"][test], labels=[\"Normal\", \"Interictal\", \"Ictal\"]))\n",
    "    print(\"training accuracy = {:.2%}, test accuracy = {:.2%}\".format(\n",
    "              accuracy_score(preds_nb[train], eeg[\"classes\"][train]),\n",
    "              accuracy_score(preds_nb[test], eeg[\"classes\"][test])))\n",
    "    print(\"training precision = {:.2%}, test precision = {:.2%}\".format(\n",
    "              precision_score(preds_nb[train], eeg[\"classes\"][train], average='weighted'),\n",
    "              precision_score(preds_nb[test], eeg[\"classes\"][test], average='weighted')))\n",
    "    print(\"training sensitivity = {:.2%}, test sensitivity = {:.2%}\".format(\n",
    "              recall_score(preds_nb[train], eeg[\"classes\"][train], average='weighted'),\n",
    "              recall_score(preds_nb[test], eeg[\"classes\"][test], average='weighted')))\n",
    "    print(\"training f1_score = {:.2%}, test f1_score = {:.2%}\".format(\n",
    "              f1_score(preds_nb[train], eeg[\"classes\"][train], average='weighted'),\n",
    "              f1_score(preds_nb[test], eeg[\"classes\"][test], average='weighted')))\n",
    "    \n",
    "    print(\"\\nK-nearest Neighbors: \")\n",
    "    print(\"confusion_matrix:\\n\", confusion_matrix(preds_knn[test], eeg[\"classes\"][test], labels=[\"Normal\", \"Interictal\", \"Ictal\"]))\n",
    "    print(\"training accuracy = {:.2%}, test accuracy = {:.2%}\".format(\n",
    "              accuracy_score(preds_knn[train], eeg[\"classes\"][train]),\n",
    "              accuracy_score(preds_knn[test], eeg[\"classes\"][test])))\n",
    "    print(\"training precision = {:.2%}, test precision = {:.2%}\".format(\n",
    "              precision_score(preds_knn[train], eeg[\"classes\"][train], average='weighted'),\n",
    "              precision_score(preds_knn[test], eeg[\"classes\"][test], average='weighted')))\n",
    "    print(\"training sensitivity = {:.2%}, test sensitivity = {:.2%}\".format(\n",
    "              recall_score(preds_knn[train], eeg[\"classes\"][train], average='weighted'),\n",
    "              recall_score(preds_knn[test], eeg[\"classes\"][test], average='weighted')))\n",
    "    print(\"training f1_score = {:.2%}, test f1_score = {:.2%}\".format(\n",
    "              f1_score(preds_knn[train], eeg[\"classes\"][train], average='weighted'),\n",
    "              f1_score(preds_knn[test], eeg[\"classes\"][test], average='weighted')))\n",
    "    \n",
    "    print(\"\\nRandom Forest Classifier: \")\n",
    "    print(\"confusion_matrix:\\n\", confusion_matrix(preds_rfc[test], eeg[\"classes\"][test], labels=[\"Normal\", \"Interictal\", \"Ictal\"]))\n",
    "    print(\"training accuracy = {:.2%}, test accuracy = {:.2%}\".format(\n",
    "              accuracy_score(preds_rfc[train], eeg[\"classes\"][train]),\n",
    "              accuracy_score(preds_rfc[test], eeg[\"classes\"][test])))\n",
    "    print(\"training precision = {:.2%}, test precision = {:.2%}\".format(\n",
    "              precision_score(preds_rfc[train], eeg[\"classes\"][train], average='weighted'),\n",
    "              precision_score(preds_rfc[test], eeg[\"classes\"][test], average='weighted')))\n",
    "    print(\"training sensitivity = {:.2%}, test sensitivity = {:.2%}\".format(\n",
    "              recall_score(preds_rfc[train], eeg[\"classes\"][train], average='weighted'),\n",
    "              recall_score(preds_rfc[test], eeg[\"classes\"][test], average='weighted'))),\n",
    "    print(\"training f1_score = {:.2%}, test f1_score = {:.2%}\".format(\n",
    "              f1_score(preds_rfc[train], eeg[\"classes\"][train], average='weighted'),\n",
    "              f1_score(preds_rfc[test], eeg[\"classes\"][test], average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <th>mean</th>\n",
       "      <th>amplitude</th>\n",
       "      <th>median</th>\n",
       "      <th>variance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>channel</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-4.132048</td>\n",
       "      <td>143.5</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>1633.048953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-52.444716</td>\n",
       "      <td>211.5</td>\n",
       "      <td>-51.0</td>\n",
       "      <td>2382.676526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.705150</td>\n",
       "      <td>165.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2222.631150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3.992433</td>\n",
       "      <td>171.5</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>2215.802969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-17.999268</td>\n",
       "      <td>170.0</td>\n",
       "      <td>-18.0</td>\n",
       "      <td>2016.994142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-27.823041</td>\n",
       "      <td>151.5</td>\n",
       "      <td>-28.0</td>\n",
       "      <td>2027.044839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-13.334635</td>\n",
       "      <td>162.0</td>\n",
       "      <td>-13.0</td>\n",
       "      <td>2139.935127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-5.653893</td>\n",
       "      <td>126.5</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>1046.012990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.200879</td>\n",
       "      <td>105.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>990.880077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-27.369539</td>\n",
       "      <td>156.0</td>\n",
       "      <td>-29.0</td>\n",
       "      <td>1909.717481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-13.600195</td>\n",
       "      <td>149.5</td>\n",
       "      <td>-13.0</td>\n",
       "      <td>1475.830881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-14.408348</td>\n",
       "      <td>138.5</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>1567.651900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7.406395</td>\n",
       "      <td>109.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1063.600769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6.127654</td>\n",
       "      <td>155.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1800.605867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-5.324628</td>\n",
       "      <td>102.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>754.237795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.012692</td>\n",
       "      <td>184.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2220.598814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8.664389</td>\n",
       "      <td>105.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>890.132666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>12.632658</td>\n",
       "      <td>195.5</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2882.546290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7.651208</td>\n",
       "      <td>104.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>974.495625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-2.070295</td>\n",
       "      <td>91.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>703.811998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-5.895289</td>\n",
       "      <td>112.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>845.370290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-5.378570</td>\n",
       "      <td>104.5</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>865.928201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5.049548</td>\n",
       "      <td>190.5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2519.290442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.684403</td>\n",
       "      <td>132.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1334.384900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5.591897</td>\n",
       "      <td>159.5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2598.472456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>15.001709</td>\n",
       "      <td>253.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3102.998533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>8.169880</td>\n",
       "      <td>130.5</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1511.963330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-49.054674</td>\n",
       "      <td>117.5</td>\n",
       "      <td>-49.0</td>\n",
       "      <td>1145.147121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2.280937</td>\n",
       "      <td>185.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1810.180532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>56.718575</td>\n",
       "      <td>218.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>2751.281551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>28.671711</td>\n",
       "      <td>1501.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>218197.690859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>-62.478155</td>\n",
       "      <td>745.5</td>\n",
       "      <td>-65.0</td>\n",
       "      <td>33297.055967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>13.618257</td>\n",
       "      <td>1330.5</td>\n",
       "      <td>-82.0</td>\n",
       "      <td>227919.561375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>-11.254821</td>\n",
       "      <td>1644.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>287067.478147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>2.630705</td>\n",
       "      <td>592.5</td>\n",
       "      <td>-13.0</td>\n",
       "      <td>49571.411827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>-2.402490</td>\n",
       "      <td>1047.5</td>\n",
       "      <td>51.0</td>\n",
       "      <td>131905.577568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>-33.119112</td>\n",
       "      <td>984.5</td>\n",
       "      <td>-28.0</td>\n",
       "      <td>86099.876952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>-15.360264</td>\n",
       "      <td>739.5</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45942.130889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>-40.771784</td>\n",
       "      <td>690.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>36606.692121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>-4.408836</td>\n",
       "      <td>342.0</td>\n",
       "      <td>-37.0</td>\n",
       "      <td>15353.760361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>-15.007811</td>\n",
       "      <td>492.0</td>\n",
       "      <td>-30.0</td>\n",
       "      <td>37316.747315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>6.172077</td>\n",
       "      <td>1336.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>205179.610370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>-5.652673</td>\n",
       "      <td>512.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>21842.903284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>28.478887</td>\n",
       "      <td>1580.5</td>\n",
       "      <td>124.0</td>\n",
       "      <td>349935.920533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>-11.387601</td>\n",
       "      <td>355.0</td>\n",
       "      <td>-34.0</td>\n",
       "      <td>11071.547105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>-5.672687</td>\n",
       "      <td>590.5</td>\n",
       "      <td>-22.0</td>\n",
       "      <td>45599.431797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>-59.404442</td>\n",
       "      <td>523.5</td>\n",
       "      <td>-38.0</td>\n",
       "      <td>19676.161298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>-21.550891</td>\n",
       "      <td>1464.0</td>\n",
       "      <td>-114.0</td>\n",
       "      <td>291865.435597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>3.958018</td>\n",
       "      <td>1146.5</td>\n",
       "      <td>-185.0</td>\n",
       "      <td>231486.099775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>3.282158</td>\n",
       "      <td>945.5</td>\n",
       "      <td>-21.0</td>\n",
       "      <td>67213.825195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>-17.420063</td>\n",
       "      <td>1725.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>299743.037850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>-33.471565</td>\n",
       "      <td>1009.5</td>\n",
       "      <td>-112.0</td>\n",
       "      <td>110398.140331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>-50.364901</td>\n",
       "      <td>852.5</td>\n",
       "      <td>-85.0</td>\n",
       "      <td>163941.262258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>9.450085</td>\n",
       "      <td>1279.5</td>\n",
       "      <td>39.0</td>\n",
       "      <td>61743.219439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>-5.800830</td>\n",
       "      <td>401.5</td>\n",
       "      <td>-16.0</td>\n",
       "      <td>15931.509270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>12.870393</td>\n",
       "      <td>876.5</td>\n",
       "      <td>83.0</td>\n",
       "      <td>110526.604876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>7.087137</td>\n",
       "      <td>433.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>25546.611641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>30.493532</td>\n",
       "      <td>1359.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>139933.730554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>37.571882</td>\n",
       "      <td>1590.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>255086.542612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>47.100073</td>\n",
       "      <td>1396.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>228947.748833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "feature       mean amplitude median       variance\n",
       "channel          0         0      0              0\n",
       "0        -4.132048     143.5   -4.0    1633.048953\n",
       "1       -52.444716     211.5  -51.0    2382.676526\n",
       "2        12.705150     165.0   13.0    2222.631150\n",
       "3        -3.992433     171.5   -4.0    2215.802969\n",
       "4       -17.999268     170.0  -18.0    2016.994142\n",
       "5       -27.823041     151.5  -28.0    2027.044839\n",
       "6       -13.334635     162.0  -13.0    2139.935127\n",
       "7        -5.653893     126.5   -5.0    1046.012990\n",
       "8         8.200879     105.0    9.0     990.880077\n",
       "9       -27.369539     156.0  -29.0    1909.717481\n",
       "10      -13.600195     149.5  -13.0    1475.830881\n",
       "11      -14.408348     138.5  -14.0    1567.651900\n",
       "12        7.406395     109.0    8.0    1063.600769\n",
       "13        6.127654     155.5    5.0    1800.605867\n",
       "14       -5.324628     102.0   -4.0     754.237795\n",
       "15        5.012692     184.0    5.0    2220.598814\n",
       "16        8.664389     105.0    9.0     890.132666\n",
       "17       12.632658     195.5   11.0    2882.546290\n",
       "18        7.651208     104.0    8.0     974.495625\n",
       "19       -2.070295      91.0   -2.0     703.811998\n",
       "20       -5.895289     112.0   -6.0     845.370290\n",
       "21       -5.378570     104.5   -6.0     865.928201\n",
       "22        5.049548     190.5    6.0    2519.290442\n",
       "23        1.684403     132.0    1.0    1334.384900\n",
       "24        5.591897     159.5    6.0    2598.472456\n",
       "25       15.001709     253.0   13.0    3102.998533\n",
       "26        8.169880     130.5    8.0    1511.963330\n",
       "27      -49.054674     117.5  -49.0    1145.147121\n",
       "28        2.280937     185.0    1.0    1810.180532\n",
       "29       56.718575     218.0   57.0    2751.281551\n",
       "..             ...       ...    ...            ...\n",
       "470      28.671711    1501.0   18.0  218197.690859\n",
       "471     -62.478155     745.5  -65.0   33297.055967\n",
       "472      13.618257    1330.5  -82.0  227919.561375\n",
       "473     -11.254821    1644.0   95.0  287067.478147\n",
       "474       2.630705     592.5  -13.0   49571.411827\n",
       "475      -2.402490    1047.5   51.0  131905.577568\n",
       "476     -33.119112     984.5  -28.0   86099.876952\n",
       "477     -15.360264     739.5   45.0   45942.130889\n",
       "478     -40.771784     690.0   11.0   36606.692121\n",
       "479      -4.408836     342.0  -37.0   15353.760361\n",
       "480     -15.007811     492.0  -30.0   37316.747315\n",
       "481       6.172077    1336.0 -100.0  205179.610370\n",
       "482      -5.652673     512.5    5.0   21842.903284\n",
       "483      28.478887    1580.5  124.0  349935.920533\n",
       "484     -11.387601     355.0  -34.0   11071.547105\n",
       "485      -5.672687     590.5  -22.0   45599.431797\n",
       "486     -59.404442     523.5  -38.0   19676.161298\n",
       "487     -21.550891    1464.0 -114.0  291865.435597\n",
       "488       3.958018    1146.5 -185.0  231486.099775\n",
       "489       3.282158     945.5  -21.0   67213.825195\n",
       "490     -17.420063    1725.0   58.0  299743.037850\n",
       "491     -33.471565    1009.5 -112.0  110398.140331\n",
       "492     -50.364901     852.5  -85.0  163941.262258\n",
       "493       9.450085    1279.5   39.0   61743.219439\n",
       "494      -5.800830     401.5  -16.0   15931.509270\n",
       "495      12.870393     876.5   83.0  110526.604876\n",
       "496       7.087137     433.0   10.0   25546.611641\n",
       "497      30.493532    1359.0   83.0  139933.730554\n",
       "498      37.571882    1590.0  116.0  255086.542612\n",
       "499      47.100073    1396.0  187.0  228947.748833\n",
       "\n",
       "[500 rows x 4 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fset_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "           n_jobs=1, penalty='l2', random_state=0, solver='lbfgs',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       " GaussianNB(priors=None),\n",
       " KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "            metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
       "            weights='uniform'),\n",
       " RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=128, n_jobs=1,\n",
       "             oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       " array([239,  59, 406, 391, 411, 485, 229, 297,  55, 293, 490, 458, 457,\n",
       "        453, 186, 194,  52,  74,  26, 488,   4, 318, 331, 245,   5, 141,\n",
       "        383, 135, 493, 122,  22,  68,  20, 382,  14, 278, 225,  64, 381,\n",
       "        231,  81, 401, 302, 499, 471, 455, 160, 478, 364, 496, 206, 319,\n",
       "         51, 306, 452, 332, 164, 106, 481,  63, 344, 427, 439, 320,  89,\n",
       "        312, 450,  93, 298, 459, 308, 395,  92,  18, 198, 145, 158, 150,\n",
       "        479, 167, 255, 230, 422,  66, 309, 253, 140, 101, 433,   2, 408,\n",
       "         17, 146, 249, 263,  30, 114, 247, 103, 405, 310, 176, 246, 116,\n",
       "        168, 415, 120, 261, 112, 360, 435, 282, 136, 190, 347, 181, 126,\n",
       "        281, 252, 407, 393, 354, 232, 133,  33, 476, 162,  34,  44,  97,\n",
       "         85,  61, 199, 268, 218,  73,  35, 303,  29, 361, 392, 443, 217,\n",
       "         27, 399, 380, 156, 429, 345, 138, 212, 104, 350, 346, 351, 215,\n",
       "        385, 189, 214, 204, 234, 259,  67,  24, 216, 223, 129, 111, 166,\n",
       "        417, 394,  40, 274, 357,  79, 313, 315,  13, 287, 409, 494, 228,\n",
       "        161,  83, 497, 473, 110, 149, 152,  16, 339, 109, 352, 139, 237,\n",
       "        260, 419, 317, 495, 400, 248, 386,  19, 328, 296, 269, 226, 414,\n",
       "          3, 378, 125, 280, 286,  77, 184, 424, 477, 275, 294, 436, 182,\n",
       "        447,  80, 307, 258,  11, 371,  86, 266,  36, 480,  58,  41, 270,\n",
       "         50, 209, 397, 410, 416, 123, 222,  62, 377, 130, 187,  23,  43,\n",
       "        441,   0, 201, 368, 426,  98, 349, 304, 487, 178, 369, 256,  94,\n",
       "        465,  95, 442, 169,  69, 305,  48, 341, 373, 207, 279, 227, 148,\n",
       "        143, 334, 180, 356, 131, 462, 498, 262, 324, 203,  84, 121, 482,\n",
       "        434, 460, 454, 384,  91,  82, 267, 119, 358, 291,  57, 321, 257,\n",
       "        376, 446,  42, 105, 388, 467, 273, 444,  38, 389,  53, 420, 437,\n",
       "        128, 290,  28, 183, 370, 163, 151, 244, 202,  31,  32, 127, 185,\n",
       "        470, 449, 288, 423, 398, 456, 147, 285, 466, 177,  99, 338, 448,\n",
       "        431, 335, 197, 243, 464, 115, 404, 265,  72, 333,  25, 165, 337,\n",
       "        483, 174, 486,  39, 193, 314, 396,  88, 472,  70,  87, 292, 242,\n",
       "        277, 211,   9, 359, 195, 251, 323, 192, 117,  47, 172]),\n",
       " array([ 90, 254, 283, 445, 461,  15, 316, 489, 159, 153, 241, 250, 390,\n",
       "        289, 171, 329, 468, 355, 154,  37, 205, 366, 240, 108,  45, 438,\n",
       "         21, 367,  96, 233, 428, 118, 124, 191, 374, 492, 311, 451, 353,\n",
       "        238, 322,  46, 403, 221,  76,   1, 213, 325, 418, 102, 363, 170,\n",
       "        343, 144, 132,  12, 327, 173, 224, 342,  78, 276, 387, 425, 301,\n",
       "        196,  10, 469, 271,  75, 142,  65, 340, 484, 175, 362, 264, 100,\n",
       "        491, 295, 300, 235, 475, 219, 330, 326, 421, 157, 348,  54, 220,\n",
       "        402, 379, 200, 179, 372,  56, 440,  60, 208, 107, 336,  71, 474,\n",
       "          6, 412, 113, 236, 299, 155, 272,   7, 137,   8, 463, 432, 375,\n",
       "        284, 210, 188, 430,  49, 134, 365, 413])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression: \n",
      "confusion_matrix:\n",
      " [[40 44  0]\n",
      " [ 7 10  4]\n",
      " [ 0  0 20]]\n",
      "training accuracy = 55.47%, test accuracy = 56.00%\n",
      "training precision = 70.60%, test precision = 73.64%\n",
      "training sensitivity = 55.47%, test sensitivity = 56.00%\n",
      "training f1_score = 59.61%, test f1_score = 60.06%\n",
      "\n",
      "Naive Bayes: \n",
      "confusion_matrix:\n",
      " [[46 44  0]\n",
      " [ 1  9  6]\n",
      " [ 0  1 18]]\n",
      "training accuracy = 54.67%, test accuracy = 58.40%\n",
      "training precision = 77.26%, test precision = 84.00%\n",
      "training sensitivity = 54.67%, test sensitivity = 58.40%\n",
      "training f1_score = 61.25%, test f1_score = 64.37%\n",
      "\n",
      "K-nearest Neighbors: \n",
      "confusion_matrix:\n",
      " [[24 27  2]\n",
      " [23 23  2]\n",
      " [ 0  4 20]]\n",
      "training accuracy = 79.73%, test accuracy = 53.60%\n",
      "training precision = 80.39%, test precision = 54.01%\n",
      "training sensitivity = 79.73%, test sensitivity = 53.60%\n",
      "training f1_score = 79.96%, test f1_score = 53.67%\n",
      "\n",
      "Random Forest Classifier: \n",
      "confusion_matrix:\n",
      " [[35  8  1]\n",
      " [12 41  2]\n",
      " [ 0  5 21]]\n",
      "training accuracy = 100.00%, test accuracy = 77.60%\n",
      "training precision = 100.00%, test precision = 77.82%\n",
      "training sensitivity = 100.00%, test sensitivity = 77.60%\n",
      "training f1_score = 100.00%, test f1_score = 77.65%\n"
     ]
    }
   ],
   "source": [
    "predict(fset_5, model_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression: \n",
      "confusion_matrix:\n",
      " [[40 44  0]\n",
      " [ 7 10  4]\n",
      " [ 0  0 20]]\n",
      "training accuracy = 55.47%, test accuracy = 56.00%\n",
      "training precision = 70.60%, test precision = 73.64%\n",
      "training sensitivity = 55.47%, test sensitivity = 56.00%\n",
      "training f1_score = 59.61%, test f1_score = 60.06%\n",
      "\n",
      "Naive Bayes: \n",
      "confusion_matrix:\n",
      " [[46 44  0]\n",
      " [ 1  9  6]\n",
      " [ 0  1 18]]\n",
      "training accuracy = 54.67%, test accuracy = 58.40%\n",
      "training precision = 77.26%, test precision = 84.00%\n",
      "training sensitivity = 54.67%, test sensitivity = 58.40%\n",
      "training f1_score = 61.25%, test f1_score = 64.37%\n",
      "\n",
      "K-nearest Neighbors: \n",
      "confusion_matrix:\n",
      " [[24 27  2]\n",
      " [23 23  2]\n",
      " [ 0  4 20]]\n",
      "training accuracy = 79.73%, test accuracy = 53.60%\n",
      "training precision = 80.39%, test precision = 54.01%\n",
      "training sensitivity = 79.73%, test sensitivity = 53.60%\n",
      "training f1_score = 79.96%, test f1_score = 53.67%\n",
      "\n",
      "Random Forest Classifier: \n",
      "confusion_matrix:\n",
      " [[35  8  1]\n",
      " [12 41  2]\n",
      " [ 0  5 21]]\n",
      "training accuracy = 100.00%, test accuracy = 77.60%\n",
      "training precision = 100.00%, test precision = 77.82%\n",
      "training sensitivity = 100.00%, test sensitivity = 77.60%\n",
      "training f1_score = 100.00%, test f1_score = 77.65%\n"
     ]
    }
   ],
   "source": [
    "predict(fset_6, model_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression: \n",
      "confusion_matrix:\n",
      " [[45 45  0]\n",
      " [ 2  8  4]\n",
      " [ 0  1 20]]\n",
      "training accuracy = 57.07%, test accuracy = 58.40%\n",
      "training precision = 76.61%, test precision = 84.60%\n",
      "training sensitivity = 57.07%, test sensitivity = 58.40%\n",
      "training f1_score = 62.47%, test f1_score = 64.87%\n",
      "\n",
      "Naive Bayes: \n",
      "confusion_matrix:\n",
      " [[46 44  0]\n",
      " [ 1  9  6]\n",
      " [ 0  1 18]]\n",
      "training accuracy = 54.67%, test accuracy = 58.40%\n",
      "training precision = 77.26%, test precision = 84.00%\n",
      "training sensitivity = 54.67%, test sensitivity = 58.40%\n",
      "training f1_score = 61.25%, test f1_score = 64.37%\n",
      "\n",
      "K-nearest Neighbors: \n",
      "confusion_matrix:\n",
      " [[24 27  2]\n",
      " [23 23  2]\n",
      " [ 0  4 20]]\n",
      "training accuracy = 79.73%, test accuracy = 53.60%\n",
      "training precision = 80.39%, test precision = 54.01%\n",
      "training sensitivity = 79.73%, test sensitivity = 53.60%\n",
      "training f1_score = 79.96%, test f1_score = 53.67%\n",
      "\n",
      "Random Forest Classifier: \n",
      "confusion_matrix:\n",
      " [[32  6  1]\n",
      " [15 44  2]\n",
      " [ 0  4 21]]\n",
      "training accuracy = 100.00%, test accuracy = 77.60%\n",
      "training precision = 100.00%, test precision = 78.51%\n",
      "training sensitivity = 100.00%, test sensitivity = 77.60%\n",
      "training f1_score = 100.00%, test f1_score = 77.70%\n"
     ]
    }
   ],
   "source": [
    "predict(fset_75, model_75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <th>amplitude</th>\n",
       "      <th>mean</th>\n",
       "      <th>skew</th>\n",
       "      <th>variance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>channel</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>143.5</td>\n",
       "      <td>-4.132048</td>\n",
       "      <td>0.032805</td>\n",
       "      <td>1633.048953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>211.5</td>\n",
       "      <td>-52.444716</td>\n",
       "      <td>-0.092715</td>\n",
       "      <td>2382.676526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>165.0</td>\n",
       "      <td>12.705150</td>\n",
       "      <td>-0.004100</td>\n",
       "      <td>2222.631150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>171.5</td>\n",
       "      <td>-3.992433</td>\n",
       "      <td>0.063678</td>\n",
       "      <td>2215.802969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>170.0</td>\n",
       "      <td>-17.999268</td>\n",
       "      <td>0.142753</td>\n",
       "      <td>2016.994142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "feature amplitude       mean      skew     variance\n",
       "channel         0          0         0            0\n",
       "0           143.5  -4.132048  0.032805  1633.048953\n",
       "1           211.5 -52.444716 -0.092715  2382.676526\n",
       "2           165.0  12.705150 -0.004100  2222.631150\n",
       "3           171.5  -3.992433  0.063678  2215.802969\n",
       "4           170.0 -17.999268  0.142753  2016.994142"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fset_5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Using Keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Reshape, Conv1D, MaxPooling1D, Flatten\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(eeg[\"measurements\"], eeg[\"classes\"], test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = np.reshape(X_train, (400,4097))\n",
    "X_test = np.reshape(X_test, (100,4097))\n",
    "\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(y_train)\n",
    "integer_encoded_test = label_encoder.fit_transform(y_test)\n",
    "\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "integer_encoded_test = integer_encoded_test.reshape(len(integer_encoded_test), 1)\n",
    "\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "onehot_encoded_test = onehot_encoder.fit_transform(integer_encoded_test)\n",
    "\n",
    "y_train = onehot_encoded\n",
    "y_train = np.reshape(y_train, (400, 1, 3))\n",
    "\n",
    "y_test = onehot_encoded_test\n",
    "y_test = np.reshape(y_test, (100, 1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, None, 32)          192       \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, None, 32)          5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, None, 32)          0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, None, 1000)        33000     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, None, 3)           3003      \n",
      "=================================================================\n",
      "Total params: 41,347\n",
      "Trainable params: 41,347\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters=32, kernel_size=5, input_shape=(None,1)))\n",
    "model.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2 ))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv1d_1_input to have 3 dimensions, but got array with shape (400, 4097)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-73a039891cbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# X_train_final = np.reshape(4097,1,400)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected conv1d_1_input to have 3 dimensions, but got array with shape (400, 4097)"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 400\n",
    "EPOCHS = 300\n",
    "\n",
    "# fset_5_array = fset_5.values\n",
    "# X_train_final = np.reshape(4097,1,400)\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5301502513885498, 0.7700342297554016]"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 90, 254, 283, 445, 461,  15, 316, 489, 159, 153, 241, 250, 390,\n",
       "       289, 171, 329, 468, 355, 154,  37, 205, 366, 240, 108,  45, 438,\n",
       "        21, 367,  96, 233, 428, 118, 124, 191, 374, 492, 311, 451, 353,\n",
       "       238, 322,  46, 403, 221,  76,   1, 213, 325, 418, 102, 363, 170,\n",
       "       343, 144, 132,  12, 327, 173, 224, 342,  78, 276, 387, 425, 301,\n",
       "       196,  10, 469, 271,  75, 142,  65, 340, 484, 175, 362, 264, 100,\n",
       "       491, 295, 300, 235, 475, 219, 330, 326, 421, 157, 348,  54, 220,\n",
       "       402, 379, 200, 179, 372,  56, 440,  60, 208, 107, 336,  71, 474,\n",
       "         6, 412, 113, 236, 299, 155, 272,   7, 137,   8, 463, 432, 375,\n",
       "       284, 210, 188, 430,  49, 134, 365, 413])"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-24.],\n",
       "        [-15.],\n",
       "        [ -5.],\n",
       "        ...,\n",
       "        [-57.],\n",
       "        [-54.],\n",
       "        [-30.]],\n",
       "\n",
       "       [[-48.],\n",
       "        [-52.],\n",
       "        [-63.],\n",
       "        ...,\n",
       "        [ -5.],\n",
       "        [ -9.],\n",
       "        [  0.]],\n",
       "\n",
       "       [[ 49.],\n",
       "        [ 54.],\n",
       "        [ 62.],\n",
       "        ...,\n",
       "        [ 12.],\n",
       "        [ 13.],\n",
       "        [ 40.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 39.],\n",
       "        [ 41.],\n",
       "        [ 43.],\n",
       "        ...,\n",
       "        [-29.],\n",
       "        [-16.],\n",
       "        [  1.]],\n",
       "\n",
       "       [[249.],\n",
       "        [218.],\n",
       "        [176.],\n",
       "        ...,\n",
       "        [367.],\n",
       "        [299.],\n",
       "        [ 26.]],\n",
       "\n",
       "       [[ 34.],\n",
       "        [ 34.],\n",
       "        [ 31.],\n",
       "        ...,\n",
       "        [ -7.],\n",
       "        [-20.],\n",
       "        [ 91.]]])"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature amplitude       mean       variance      skew\n",
      "channel         0          0              0         0\n",
      "90          144.5 -28.239688    1987.303058 -0.046706\n",
      "254          99.0  -8.651452     916.789913 -0.347122\n",
      "283         254.0 -52.632170    4541.735582 -0.054374\n",
      "445         450.0   6.621186   20847.731531 -0.299523\n",
      "461         246.0  -7.891140   11268.679387  0.303489\n",
      "15          184.0   5.012692    2220.598814 -0.195767\n",
      "316         114.5  -3.758848    1391.233034 -0.257820\n",
      "489         945.5   3.282158   67213.825195  0.216552\n",
      "159         342.0 -19.309007    9676.057310  0.134387\n",
      "153         141.5  56.330242    1522.331019  0.182110\n",
      "241          65.5  -8.377593     346.218419 -0.317217\n",
      "250         405.5   0.377105    9239.074047  0.116386\n",
      "390         275.0   5.626312    7764.221353 -0.061711\n",
      "289          92.5  -3.938492     517.421650 -0.434865\n",
      "171         151.0   2.873810    1704.686785 -0.007218\n",
      "329         528.5 -38.859409   20723.485726  2.307538\n",
      "468         335.5 -53.463754   11322.854984  1.107440\n",
      "355         128.5  10.959727    1522.550245 -0.055996\n",
      "154         240.0   6.600928    4445.854166  0.202806\n",
      "37           77.5 -75.103734     534.277499  0.076754\n",
      "205         234.0  -3.262631    4914.709643  0.185728\n",
      "366         182.5 -15.857701    2365.160859  0.000418\n",
      "240         302.0  -7.396144    4750.624862 -0.271712\n",
      "108         203.5  -2.998291    3858.252621 -0.087986\n",
      "45          156.0 -18.813522    1575.992075  0.050949\n",
      "438        1012.0 -15.138882   85706.248469  0.489576\n",
      "21          104.5  -5.378570     865.928201  0.009344\n",
      "367         116.5  28.355138    1170.525329 -0.058368\n",
      "96          149.5  -3.159873    1699.131629 -0.188527\n",
      "233         102.5 -11.803271     804.132642 -0.273388\n",
      "..            ...        ...            ...       ...\n",
      "372         608.0 -30.079326   34623.541913  0.217686\n",
      "56          176.5   1.610935    2480.139085  0.073890\n",
      "440        1409.5  -8.122529  243626.097752 -1.580058\n",
      "60           75.5 -75.503783     511.163825 -0.071865\n",
      "208         293.5 -37.617525    5558.248880 -0.029854\n",
      "107         216.5   9.191848    4411.932440  0.049315\n",
      "336         178.5  23.210398    2277.455367 -0.025972\n",
      "71          153.5   6.471809    2042.409811  0.046354\n",
      "474         592.5   2.630705   49571.411827  0.192447\n",
      "6           162.0 -13.334635    2139.935127  0.003179\n",
      "412         470.0 -16.205760   34219.651829 -0.420786\n",
      "113         241.0   6.122773    4198.850682  0.026963\n",
      "236         184.5  -8.330242    2392.659552 -0.060674\n",
      "299         163.0  -6.296803    2604.640734 -0.187975\n",
      "155         262.0 -50.904564    5768.750716  0.343450\n",
      "272          86.5  -6.214791     351.634119 -0.279301\n",
      "7           126.5  -5.653893    1046.012990 -0.023597\n",
      "137         208.0  -3.186478    3622.324513  0.055949\n",
      "8           105.0   8.200879     990.880077 -0.002426\n",
      "463        1609.0  -5.442763  349235.628711 -1.256881\n",
      "432        1711.5 -10.084696  309876.509058 -1.043538\n",
      "375         736.0 -51.764706   50007.440853  2.661113\n",
      "284         157.0  28.515743    1934.321512 -1.223013\n",
      "210         112.5 -12.512326     965.169545 -0.260101\n",
      "188         195.5  -1.548206    2117.883995  0.128147\n",
      "430         527.5 -67.334147   35170.534916  0.425622\n",
      "49          184.5   3.820356    2488.566703 -0.055685\n",
      "134         196.0   2.203320    3293.202254  0.114851\n",
      "365         182.0 -16.297779    2961.872275 -0.952198\n",
      "413         948.0  25.535026  140352.575842 -0.864777\n",
      "\n",
      "[125 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(fset_5.iloc[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(eeg[\"measurements\"], eeg[\"classes\"], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.reshape(X_test, (4097,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-24., -15.,  -5., ..., -36., -33., -32.],\n",
       "       [-31., -37., -52., ..., 112., 118., 118.],\n",
       "       [116., 118., 120., ...,  28.,  27.,  31.],\n",
       "       ...,\n",
       "       [ 18.,  30.,  17., ..., -29., -43., -53.],\n",
       "       [-65., -75., -71., ..., -61., -44., -27.],\n",
       "       [-11.,  -5.,  -7., ...,  -7., -20.,  91.]])"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 90, 254, 283, 445, 461,  15, 316, 489, 159, 153, 241, 250, 390,\n",
       "       289, 171, 329, 468, 355, 154,  37, 205, 366, 240, 108,  45, 438,\n",
       "        21, 367,  96, 233, 428, 118, 124, 191, 374, 492, 311, 451, 353,\n",
       "       238, 322,  46, 403, 221,  76,   1, 213, 325, 418, 102, 363, 170,\n",
       "       343, 144, 132,  12, 327, 173, 224, 342,  78, 276, 387, 425, 301,\n",
       "       196,  10, 469, 271,  75, 142,  65, 340, 484, 175, 362, 264, 100,\n",
       "       491, 295, 300, 235, 475, 219, 330, 326, 421, 157, 348,  54, 220,\n",
       "       402, 379, 200, 179, 372,  56, 440,  60, 208, 107, 336,  71, 474,\n",
       "         6, 412, 113, 236, 299, 155, 272,   7, 137,   8, 463, 432, 375,\n",
       "       284, 210, 188, 430,  49, 134, 365, 413])"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <th>amplitude</th>\n",
       "      <th>mean</th>\n",
       "      <th>variance</th>\n",
       "      <th>skew</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>channel</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>984.5</td>\n",
       "      <td>-33.119112</td>\n",
       "      <td>86099.876952</td>\n",
       "      <td>-0.049577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>590.5</td>\n",
       "      <td>-5.672687</td>\n",
       "      <td>45599.431797</td>\n",
       "      <td>0.288864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>876.5</td>\n",
       "      <td>12.870393</td>\n",
       "      <td>110526.604876</td>\n",
       "      <td>-0.472757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>1396.0</td>\n",
       "      <td>47.100073</td>\n",
       "      <td>228947.748833</td>\n",
       "      <td>-1.347758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>170.0</td>\n",
       "      <td>-17.999268</td>\n",
       "      <td>2016.994142</td>\n",
       "      <td>0.142753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>171.5</td>\n",
       "      <td>-3.992433</td>\n",
       "      <td>2215.802969</td>\n",
       "      <td>0.063678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>162.0</td>\n",
       "      <td>-13.334635</td>\n",
       "      <td>2139.935127</td>\n",
       "      <td>0.003179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>149.5</td>\n",
       "      <td>-13.600195</td>\n",
       "      <td>1475.830881</td>\n",
       "      <td>-0.120190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>138.5</td>\n",
       "      <td>-14.408348</td>\n",
       "      <td>1567.651900</td>\n",
       "      <td>-0.091485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>126.5</td>\n",
       "      <td>-5.653893</td>\n",
       "      <td>1046.012990</td>\n",
       "      <td>-0.023597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>105.0</td>\n",
       "      <td>8.200879</td>\n",
       "      <td>990.880077</td>\n",
       "      <td>-0.002426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>109.0</td>\n",
       "      <td>7.406395</td>\n",
       "      <td>1063.600769</td>\n",
       "      <td>0.005112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>149.5</td>\n",
       "      <td>-13.600195</td>\n",
       "      <td>1475.830881</td>\n",
       "      <td>-0.120190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>149.5</td>\n",
       "      <td>-13.600195</td>\n",
       "      <td>1475.830881</td>\n",
       "      <td>-0.120190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>151.5</td>\n",
       "      <td>-27.823041</td>\n",
       "      <td>2027.044839</td>\n",
       "      <td>0.002501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>1396.0</td>\n",
       "      <td>47.100073</td>\n",
       "      <td>228947.748833</td>\n",
       "      <td>-1.347758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>945.5</td>\n",
       "      <td>3.282158</td>\n",
       "      <td>67213.825195</td>\n",
       "      <td>0.216552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>1464.0</td>\n",
       "      <td>-21.550891</td>\n",
       "      <td>291865.435597</td>\n",
       "      <td>0.180866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>984.5</td>\n",
       "      <td>-33.119112</td>\n",
       "      <td>86099.876952</td>\n",
       "      <td>-0.049577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>246.0</td>\n",
       "      <td>-7.891140</td>\n",
       "      <td>11268.679387</td>\n",
       "      <td>0.303489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>1128.5</td>\n",
       "      <td>56.176227</td>\n",
       "      <td>148112.626499</td>\n",
       "      <td>0.573022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>1074.5</td>\n",
       "      <td>-2.815963</td>\n",
       "      <td>63062.828713</td>\n",
       "      <td>-0.110249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>707.0</td>\n",
       "      <td>-31.137662</td>\n",
       "      <td>72841.483124</td>\n",
       "      <td>0.698109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>1672.5</td>\n",
       "      <td>-16.371735</td>\n",
       "      <td>244899.009726</td>\n",
       "      <td>-0.735545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>375.5</td>\n",
       "      <td>-13.025873</td>\n",
       "      <td>12030.419638</td>\n",
       "      <td>-0.147064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>1343.5</td>\n",
       "      <td>-5.947034</td>\n",
       "      <td>188225.527827</td>\n",
       "      <td>-0.647590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>1128.5</td>\n",
       "      <td>56.176227</td>\n",
       "      <td>148112.626499</td>\n",
       "      <td>0.573022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>1672.5</td>\n",
       "      <td>-16.371735</td>\n",
       "      <td>244899.009726</td>\n",
       "      <td>-0.735545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>1672.5</td>\n",
       "      <td>-16.371735</td>\n",
       "      <td>244899.009726</td>\n",
       "      <td>-0.735545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>1023.0</td>\n",
       "      <td>-9.378814</td>\n",
       "      <td>53289.446932</td>\n",
       "      <td>0.114365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>527.5</td>\n",
       "      <td>-67.334147</td>\n",
       "      <td>35170.534916</td>\n",
       "      <td>0.425622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>707.0</td>\n",
       "      <td>-31.137662</td>\n",
       "      <td>72841.483124</td>\n",
       "      <td>0.698109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>852.5</td>\n",
       "      <td>-18.520137</td>\n",
       "      <td>123207.981594</td>\n",
       "      <td>-0.753019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>1464.0</td>\n",
       "      <td>-21.550891</td>\n",
       "      <td>291865.435597</td>\n",
       "      <td>0.180866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>143.5</td>\n",
       "      <td>-4.132048</td>\n",
       "      <td>1633.048953</td>\n",
       "      <td>0.032805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>102.0</td>\n",
       "      <td>-5.324628</td>\n",
       "      <td>754.237795</td>\n",
       "      <td>-0.281320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>132.0</td>\n",
       "      <td>1.684403</td>\n",
       "      <td>1334.384900</td>\n",
       "      <td>0.054414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>130.5</td>\n",
       "      <td>8.169880</td>\n",
       "      <td>1511.963330</td>\n",
       "      <td>0.008883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>130.5</td>\n",
       "      <td>8.169880</td>\n",
       "      <td>1511.963330</td>\n",
       "      <td>0.008883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>104.0</td>\n",
       "      <td>7.651208</td>\n",
       "      <td>974.495625</td>\n",
       "      <td>-0.047926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>155.5</td>\n",
       "      <td>6.127654</td>\n",
       "      <td>1800.605867</td>\n",
       "      <td>0.055195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>162.0</td>\n",
       "      <td>-13.334635</td>\n",
       "      <td>2139.935127</td>\n",
       "      <td>0.003179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>105.0</td>\n",
       "      <td>8.200879</td>\n",
       "      <td>990.880077</td>\n",
       "      <td>-0.002426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>149.5</td>\n",
       "      <td>-13.600195</td>\n",
       "      <td>1475.830881</td>\n",
       "      <td>-0.120190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>105.0</td>\n",
       "      <td>8.664389</td>\n",
       "      <td>890.132666</td>\n",
       "      <td>0.050575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>112.0</td>\n",
       "      <td>-5.895289</td>\n",
       "      <td>845.370290</td>\n",
       "      <td>0.197725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>104.5</td>\n",
       "      <td>-5.378570</td>\n",
       "      <td>865.928201</td>\n",
       "      <td>0.009344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>104.0</td>\n",
       "      <td>7.651208</td>\n",
       "      <td>974.495625</td>\n",
       "      <td>-0.047926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>184.0</td>\n",
       "      <td>5.012692</td>\n",
       "      <td>2220.598814</td>\n",
       "      <td>-0.195767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>184.0</td>\n",
       "      <td>5.012692</td>\n",
       "      <td>2220.598814</td>\n",
       "      <td>-0.195767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>156.0</td>\n",
       "      <td>-27.369539</td>\n",
       "      <td>1909.717481</td>\n",
       "      <td>0.202650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>162.0</td>\n",
       "      <td>-13.334635</td>\n",
       "      <td>2139.935127</td>\n",
       "      <td>0.003179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>165.0</td>\n",
       "      <td>12.705150</td>\n",
       "      <td>2222.631150</td>\n",
       "      <td>-0.004100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>165.0</td>\n",
       "      <td>12.705150</td>\n",
       "      <td>2222.631150</td>\n",
       "      <td>-0.004100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>876.5</td>\n",
       "      <td>12.870393</td>\n",
       "      <td>110526.604876</td>\n",
       "      <td>-0.472757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>1464.0</td>\n",
       "      <td>-21.550891</td>\n",
       "      <td>291865.435597</td>\n",
       "      <td>0.180866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>1047.5</td>\n",
       "      <td>-2.402490</td>\n",
       "      <td>131905.577568</td>\n",
       "      <td>-0.755748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>563.0</td>\n",
       "      <td>10.474982</td>\n",
       "      <td>35506.141490</td>\n",
       "      <td>-0.024357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>852.5</td>\n",
       "      <td>-18.520137</td>\n",
       "      <td>123207.981594</td>\n",
       "      <td>-0.753019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>335.5</td>\n",
       "      <td>-53.463754</td>\n",
       "      <td>11322.854984</td>\n",
       "      <td>1.107440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "feature amplitude       mean       variance      skew\n",
       "channel         0          0              0         0\n",
       "476         984.5 -33.119112   86099.876952 -0.049577\n",
       "485         590.5  -5.672687   45599.431797  0.288864\n",
       "495         876.5  12.870393  110526.604876 -0.472757\n",
       "499        1396.0  47.100073  228947.748833 -1.347758\n",
       "4           170.0 -17.999268    2016.994142  0.142753\n",
       "3           171.5  -3.992433    2215.802969  0.063678\n",
       "6           162.0 -13.334635    2139.935127  0.003179\n",
       "10          149.5 -13.600195    1475.830881 -0.120190\n",
       "11          138.5 -14.408348    1567.651900 -0.091485\n",
       "7           126.5  -5.653893    1046.012990 -0.023597\n",
       "8           105.0   8.200879     990.880077 -0.002426\n",
       "12          109.0   7.406395    1063.600769  0.005112\n",
       "10          149.5 -13.600195    1475.830881 -0.120190\n",
       "10          149.5 -13.600195    1475.830881 -0.120190\n",
       "5           151.5 -27.823041    2027.044839  0.002501\n",
       "499        1396.0  47.100073  228947.748833 -1.347758\n",
       "489         945.5   3.282158   67213.825195  0.216552\n",
       "487        1464.0 -21.550891  291865.435597  0.180866\n",
       "476         984.5 -33.119112   86099.876952 -0.049577\n",
       "461         246.0  -7.891140   11268.679387  0.303489\n",
       "456        1128.5  56.176227  148112.626499  0.573022\n",
       "448        1074.5  -2.815963   63062.828713 -0.110249\n",
       "450         707.0 -31.137662   72841.483124  0.698109\n",
       "451        1672.5 -16.371735  244899.009726 -0.735545\n",
       "457         375.5 -13.025873   12030.419638 -0.147064\n",
       "459        1343.5  -5.947034  188225.527827 -0.647590\n",
       "456        1128.5  56.176227  148112.626499  0.573022\n",
       "451        1672.5 -16.371735  244899.009726 -0.735545\n",
       "451        1672.5 -16.371735  244899.009726 -0.735545\n",
       "447        1023.0  -9.378814   53289.446932  0.114365\n",
       "..            ...        ...            ...       ...\n",
       "430         527.5 -67.334147   35170.534916  0.425622\n",
       "450         707.0 -31.137662   72841.483124  0.698109\n",
       "467         852.5 -18.520137  123207.981594 -0.753019\n",
       "487        1464.0 -21.550891  291865.435597  0.180866\n",
       "0           143.5  -4.132048    1633.048953  0.032805\n",
       "14          102.0  -5.324628     754.237795 -0.281320\n",
       "23          132.0   1.684403    1334.384900  0.054414\n",
       "26          130.5   8.169880    1511.963330  0.008883\n",
       "26          130.5   8.169880    1511.963330  0.008883\n",
       "18          104.0   7.651208     974.495625 -0.047926\n",
       "13          155.5   6.127654    1800.605867  0.055195\n",
       "6           162.0 -13.334635    2139.935127  0.003179\n",
       "8           105.0   8.200879     990.880077 -0.002426\n",
       "10          149.5 -13.600195    1475.830881 -0.120190\n",
       "16          105.0   8.664389     890.132666  0.050575\n",
       "20          112.0  -5.895289     845.370290  0.197725\n",
       "21          104.5  -5.378570     865.928201  0.009344\n",
       "18          104.0   7.651208     974.495625 -0.047926\n",
       "15          184.0   5.012692    2220.598814 -0.195767\n",
       "15          184.0   5.012692    2220.598814 -0.195767\n",
       "9           156.0 -27.369539    1909.717481  0.202650\n",
       "6           162.0 -13.334635    2139.935127  0.003179\n",
       "2           165.0  12.705150    2222.631150 -0.004100\n",
       "2           165.0  12.705150    2222.631150 -0.004100\n",
       "495         876.5  12.870393  110526.604876 -0.472757\n",
       "487        1464.0 -21.550891  291865.435597  0.180866\n",
       "475        1047.5  -2.402490  131905.577568 -0.755748\n",
       "464         563.0  10.474982   35506.141490 -0.024357\n",
       "467         852.5 -18.520137  123207.981594 -0.753019\n",
       "468         335.5 -53.463754   11322.854984  1.107440\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fset_5.iloc[X_test[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-24., -15.,  -5., ..., -57., -54., -30.],\n",
       "       [-48., -52., -63., ...,  -5.,  -9.,   0.],\n",
       "       [ 49.,  54.,  62., ...,  12.,  13.,  40.],\n",
       "       ...,\n",
       "       [ 39.,  41.,  43., ..., -29., -16.,   1.],\n",
       "       [249., 218., 176., ..., 367., 299.,  26.],\n",
       "       [ 34.,  34.,  31., ...,  -7., -20.,  91.]])"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
